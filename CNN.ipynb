{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b23f1ae6-9d1d-41c6-b882-97ed841ec0d3",
   "metadata": {},
   "source": [
    "# Belal Khaled - 2136873\n",
    "## NN & Deep Learning Assignment 03: \n",
    "### \" CNN \".\n",
    "\n",
    "# ـــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b463c046-e0b0-468b-bacc-d7f9b00bc4a5",
   "metadata": {},
   "source": [
    "# • Import all Libraries we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7c14bfe-6fb0-4374-9a48-1510a8e3490b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Convolution2D\n",
    "from tensorflow.keras.layers import MaxPool2D\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679ad93b-07a6-4060-bf3c-a57bb9c8845e",
   "metadata": {},
   "source": [
    "# • Specifying the folder where images are present:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72773f04-bf71-4c9e-90d5-bf410fd1dbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainingImagePath = 'Final Training Images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86aa92dc-3253-420c-a261-52de7081a14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "TestingImagePath = 'Final Testing Images'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c97df7-697b-47a8-900f-3accc75b5e2b",
   "metadata": {},
   "source": [
    "# • Defining pre-processing transformations on raw images of training data and testing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03f19be6-46e3-4fa8-b294-73923856e1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "        shear_range=0.1,\n",
    "        zoom_range=0.1,\n",
    "        horizontal_flip=True)\n",
    " \n",
    "test_datagen = ImageDataGenerator()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a378c7e-3f75-40e7-a2c2-ff72b8852790",
   "metadata": {},
   "source": [
    "# • Generating the Training Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1edb9d88-32dd-4ec5-a5e8-16ef7db61434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 244 images belonging to 16 classes.\n"
     ]
    }
   ],
   "source": [
    "training_set = train_datagen.flow_from_directory(\n",
    "        TrainingImagePath,\n",
    "        target_size=(64, 64),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc26173-8749-4cc7-b255-265d9b6c9dff",
   "metadata": {},
   "source": [
    "# • Generating the Testing Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fb1f914-dc87-4e87-8882-79380503291f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 64 images belonging to 16 classes.\n"
     ]
    }
   ],
   "source": [
    "test_set = test_datagen.flow_from_directory(\n",
    "        TestingImagePath,\n",
    "        target_size=(64, 64),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ba4258-4239-4463-8a78-d4dfaf0ecbc2",
   "metadata": {},
   "source": [
    "# • Printing class labels for each face:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93f4b19f-f9ce-4b1d-b418-0809fbcd3395",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'face1': 0,\n",
       " 'face10': 1,\n",
       " 'face11': 2,\n",
       " 'face12': 3,\n",
       " 'face13': 4,\n",
       " 'face14': 5,\n",
       " 'face15': 6,\n",
       " 'face16': 7,\n",
       " 'face2': 8,\n",
       " 'face3': 9,\n",
       " 'face4': 10,\n",
       " 'face5': 11,\n",
       " 'face6': 12,\n",
       " 'face7': 13,\n",
       " 'face8': 14,\n",
       " 'face9': 15}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set.class_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582f9750-d4fc-4644-b0fb-b28d53a153d3",
   "metadata": {},
   "source": [
    "# ـــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df24401-7678-46b0-8c8e-44701194c809",
   "metadata": {},
   "source": [
    "# • Creating a mapping for index and face names:\n",
    "### ○ class_indices have the numeric tag for each face:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8414751a-b18e-4624-97e6-f67167f51382",
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainClasses=training_set.class_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc7af45-fd46-44d7-a7bf-15d9afb3b78e",
   "metadata": {},
   "source": [
    "# • Storing the face and the numeric tag for future reference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf6b66b8-f551-4057-a017-d965a90e2f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "ResultMap={}\n",
    "for faceValue,faceName in zip(TrainClasses.values(),TrainClasses.keys()):\n",
    "    ResultMap[faceValue]=faceName"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85248c10-a4db-480b-af55-1bcb538f9f45",
   "metadata": {},
   "source": [
    "# • Saving the face map for future reference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa1dcc78-e70b-4afa-b3d2-fef083bab481",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ResultsMap.pkl\", 'wb') as fileWriteStream:\n",
    "    pickle.dump(ResultMap, fileWriteStream)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d808181f-b321-4119-ad0b-38f620ff6664",
   "metadata": {},
   "source": [
    "# • The model will give answer as a numeric tag:\n",
    "### ○ This mapping will help to get the corresponding face name for it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d17a2f4f-f224-4710-837c-a1ea7dea54cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping of Face and its ID {0: 'face1', 1: 'face10', 2: 'face11', 3: 'face12', 4: 'face13', 5: 'face14', 6: 'face15', 7: 'face16', 8: 'face2', 9: 'face3', 10: 'face4', 11: 'face5', 12: 'face6', 13: 'face7', 14: 'face8', 15: 'face9'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Mapping of Face and its ID\",ResultMap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec7c3b1-13c5-4f01-b49e-38cfed246f11",
   "metadata": {},
   "source": [
    "# • The number of neurons for the output layer is equal to the number of faces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4eb6bc82-a89f-43b5-a506-1ed64bda03ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " The Number of output neurons:  16\n"
     ]
    }
   ],
   "source": [
    "OutputNeurons=len(ResultMap)\n",
    "print('\\n The Number of output neurons: ', OutputNeurons)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1c82e9-c84e-4fdc-8495-0d856afe8c7a",
   "metadata": {},
   "source": [
    "# ـــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0bcb67-f385-4643-9116-1f6b3dc604ff",
   "metadata": {},
   "source": [
    "# • Creating the CNN face recognition model:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fba88df-86ae-424e-b235-248ca2e71cf3",
   "metadata": {},
   "source": [
    "### ○ Initializing the Convolutional Neural Network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc0b9da1-baa4-45e9-898c-2c6500b5c875",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier= Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15df1c3c-851a-4f75-b55d-f9470a1f1ca4",
   "metadata": {},
   "source": [
    "# • STEP--1 Convolution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61aea786-5cbc-46b8-9bcc-b8931094402d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MIS\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "classifier.add(Convolution2D(32, kernel_size=(5, 5), strides=(1, 1), input_shape=(64,64,3), activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13216881-ab2b-4f00-b4a6-d46aaea21423",
   "metadata": {},
   "source": [
    "# • STEP--2 MAX Pooling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec9e0afa-8e96-4673-ae12-2d9bfb4b65d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(MaxPool2D(pool_size=(2,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88eb1a6f-11cd-4cef-a3a6-66949038207e",
   "metadata": {},
   "source": [
    "# • ADDITIONAL LAYER of CONVOLUTION for better accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb09b481-f923-42f8-bd29-0e8eb648e04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(Convolution2D(64, kernel_size=(5, 5), strides=(1, 1), activation='relu'))\n",
    "classifier.add(MaxPool2D(pool_size=(2,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceaa72c4-0591-4b6c-8f88-16e3fea7ac8e",
   "metadata": {},
   "source": [
    "# • STEP--3 FLattening:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73ead85c-432e-48c7-a8d8-8a625cd7718a",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e9cc6a-e998-47dc-9cc9-304c3f2c7e32",
   "metadata": {},
   "source": [
    "# • STEP--4 Fully Connected Neural Network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "12e80732-5b76-4717-a1cf-d7775f2ebed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(Dense(64, activation='relu'))\n",
    "classifier.add(Dense(OutputNeurons, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a9b6f1-2284-43d7-a36d-c6bce0799691",
   "metadata": {},
   "source": [
    "# • Compiling the CNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ed3b63f9-b5e0-454b-9d6e-65369de21e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.compile(loss='categorical_crossentropy', optimizer = 'adam', metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9402240-e4d9-4999-afce-cbeff72cb667",
   "metadata": {},
   "source": [
    "# ـــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04db6151-6df9-42e5-a76f-c89589e0d134",
   "metadata": {},
   "source": [
    "# • Measuring the time taken by the model to train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7a062946-98d3-4079-a314-c4c3e8a6f7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "StartTime=time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cf2704-fed1-4ef8-8786-ce72a35319aa",
   "metadata": {},
   "source": [
    "# • Starting the model training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a78f6d78-a9f0-4901-a1a7-d4388e9700d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MIS\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 8/30\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.0874 - loss: 139.6080"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MIS\\anaconda3\\lib\\contextlib.py:137: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - accuracy: 0.0774 - loss: 109.7382 - val_accuracy: 0.1094 - val_loss: 2.8210\n",
      "Epoch 2/10\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.1004 - loss: 2.8187 - val_accuracy: 0.1406 - val_loss: 2.6352\n",
      "Epoch 3/10\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.2191 - loss: 2.5248 - val_accuracy: 0.3750 - val_loss: 2.2355\n",
      "Epoch 4/10\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.4597 - loss: 1.9439 - val_accuracy: 0.6094 - val_loss: 1.4738\n",
      "Epoch 5/10\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.6357 - loss: 1.2502 - val_accuracy: 0.8281 - val_loss: 0.6344\n",
      "Epoch 6/10\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.8061 - loss: 0.6919 - val_accuracy: 0.8438 - val_loss: 0.6013\n",
      "Epoch 7/10\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9215 - loss: 0.3560 - val_accuracy: 0.8906 - val_loss: 0.3891\n",
      "Epoch 8/10\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.9277 - loss: 0.2947 - val_accuracy: 0.9375 - val_loss: 0.2794\n",
      "Epoch 9/10\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9615 - loss: 0.1618 - val_accuracy: 0.9219 - val_loss: 0.3133\n",
      "Epoch 10/10\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9684 - loss: 0.1093 - val_accuracy: 0.9375 - val_loss: 0.2007\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1f29b346c70>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(\n",
    "                    training_set,\n",
    "                    steps_per_epoch=30,\n",
    "                    epochs=10,\n",
    "                    validation_data=test_set,\n",
    "                    validation_steps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "14814bd5-c789-484d-b445-52176b388b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Total Time Taken:  0 Minutes \n"
     ]
    }
   ],
   "source": [
    "EndTime=time.time()\n",
    "print(\" Total Time Taken: \", round((EndTime-StartTime)/60), 'Minutes ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6d71db-97a0-411b-8d18-c3c91c97bed7",
   "metadata": {},
   "source": [
    "# ـــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f9a33f-4e55-4137-baea-2d392f3d508b",
   "metadata": {},
   "source": [
    "# • Testing the CNN classifier on unseen images:\n",
    "### ○ Making single predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ffcfab5b-232a-4831-8531-891b8f3535f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ImagePath='Final Testing Images/face7/3face7.jpg'\n",
    "test_image=image.load_img(ImagePath,target_size=(64, 64))\n",
    "test_image=image.img_to_array(test_image)\n",
    "\n",
    "test_image=np.expand_dims(test_image,axis=0)\n",
    "\n",
    "\n",
    "result=classifier.predict(test_image,verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95521dad-dd2f-4e97-a79b-bb8104d921dc",
   "metadata": {},
   "source": [
    "# • Predicting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "74ba2957-2a12-4b8d-aaa7-df155536aa1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction is:  face7\n"
     ]
    }
   ],
   "source": [
    "print('Prediction is: ',ResultMap[np.argmax(result)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b54ded6-6bad-462e-ab60-2f032e706143",
   "metadata": {},
   "source": [
    "# ـــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــــ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
